# =========================
# DATASET CONFIGURATION
# =========================
dataset:
  train_csv: "data/us/train.csv"
  val_csv: "data/us/dev.csv"
  pose_pkl: "data/pose_data_isharah1000_hands_lips_body_May12.pkl"
  delimiter: ","
  min_len: 32
  max_len: 1000
  additional_joints: true
  augment: true
  augment_prob: 0.5
  tokenizer_type: "seq2seq"   # "ctc" hoặc "seq2seq"

# =========================
# MODEL CONFIGURATION
# =========================
model:
  name: "BaselineAttentionLinear"
  input_dim: 172   # Số joints * 2 (nếu pose là [B, T, 43, 2] thì 43*2=86)
  hidden_dim: 512
  num_layers: 2
  num_heads: 8
  conv_channels: 512
  mlp_hidden: 512
  num_decoder_layers: 1
  # dec_mlp_ratio: 4
  dropout: 0.1
  max_input_len: 1000    # <-- Độ dài sequence đầu vào (encoder)
  max_output_len: 12    # <-- Độ dài sequence đầu ra (decode/inference)

# =========================
# TRAINER CONFIGURATION
# =========================
trainer:
  batch_size: 32
  num_workers: 8
  epochs: 300
  lr: 0.0005
  weight_decay: 0.01
  warmup_pct: 0.1
  device: "cuda"
  seed: 42
  # save_dir: "./checkpoints/"
  
  encoder_pretrained: "./logsus/CSLRTransformerBaselineIndependent/runs_20250605_161009/version_0/checkpoints/last.ckpt"
  # encode
  log_interval: 50
  # log_interval_step: 100
  check_val_every_n_epoch: 1
  devices: 1
# =========================
# LOGGING & MISC
# =========================
logging:
  log_dir: "./logs"
  print_config: true
